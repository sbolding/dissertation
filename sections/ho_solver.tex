
\chapter{\uppercase {The Exponentially-Convergent Monte Carlo High-Order Solver}}

The transport equation to be solved by the HO solver is
\begin{equation}\label{eq:ho_base}
\mu \pderiv{I^{n+1,k+1/2}}{x} + \left(\sigma_t^k + \frac{1}{c \Delta t }\right)
I^{n+1,k+1/2}
= \frac{\sigma_s}{2} \phi^{n+1,k} +\frac{1}{2} \left(\sigma_a^k a c T^4
\right)^{n+1,k} + \frac{\tilde I^n}{c\Delta t} 
\end{equation}
where the superscript $k$ represents the outer HOLO iteration index.  Here, $k+1/2$ denotes the
HO solve within outer HOLO iteration $k$, whereas $k$ and $k+1$ represent successive LO
solves. The sources at $k$ in Eq.~\eqref{eq:ho_base} are estimated by the previous LO
solution. Temperature-dependent cross sections are
evaluated at $T^{n+1,k}$.  As all sources on the right side of the equation are known,
this defines a fixed-source, pure absorber transport problem.  The above transport equation has
the same form as a steady-state neutronics problem.  We will solve
this transport problem using the ECMC method. 

In the remainder of this chapter, an overview of
the ECMC solution method applied in this work is given. A more detailed description of the
algorithm for neutrnoics problems can be found in~\cite{jake}, but an overview of the algorithm, sampling, and
trial space are given here.  Relevant details and modifications made to the
algorithm for this work are given herein.

\section{The ECMC Algorithm}

The ECMC method is an iterative residual MC method. 
In operator notation, Eq.~\eqref{eq:ho_base} can be written as
\begin{equation}\label{te_oper}
\B L^k I^{n+1,k+1/2}  = q^{k}
\end{equation}
where $I^{n+1,k+1/2}$ is the transport solution of the angular intensity based on the
$k$-th LO estimate of $q^k$.
The linear operator $\B L^k$ is the \emph{continuous} streaming plus
removal operator, given by the left-hand
side of Eq.~\eqref{eq:ho_base}, i.e.,
\begin{equation}
    \B L^k(\cdot) = \left[\mu \pderiv{}{x} + \left(\sigma_t^k + \frac{1}{c \Delta t
    }\right)\right] \left(\cdot\right)
\end{equation}
We will use superscript $(m)$ to indicated the $m$-th inner HO iteration.  The LDFE
representation of the $m$-th
approximate solution to Eq.~\eqref{te_oper} is denoted
$\tilde{I}^{n+1,(m)}(x,\mu)$.    
The associated residual is defined as $r^{(m)} = q - \B L^k\tilde{I}^{n+1,(m)}.$ 
Explicitly, the residual at iteration $m$ is
\begin{multline}\label{eq:resid}
r^{(m),k+1/2} = \frac{\sigma_s}{2} \phi^{n+1,k} +\frac{1}{2} \left(\sigma_a a c T^4
\right)^{n+1,k} + \frac{\tilde{I}^n}{c \Delta t } \\ -
\left(\mu \pderiv{\tilde{I}^{n+1,k+1/2}}{x} +
\left(\sigma_t^k + \frac{1}{c \Delta t }\right) \tilde{I}^{n+1,k+1/2}\right)^{(m)}
\end{multline}
where the $k$ terms have a LDFE representation in space on the coarsest mesh and are not recalculated at any point during
the HO solve. The functional form of $\tilde{I}^n$ is defined from the final HO
solution of the previous time step.  The HOLO iteration indices are suppressed for the
remainder of this chapter because the LO-estimated $q^{k}$ and $\B L^{k}$
\emph{remain constant for the entire HO solve}.

Addition of $\B L I^{n+1} - q=0$ to the Eq.~\eqref{eq:resid}, i.e., the residual equation,
and manipulation of the result yields the error equation
\begin{equation}\label{eq:err_eq}
    \B L (I^{n+1} - \tilde{I}^{n+1,(m)}) = \B L {\epsilon}^{(m)} = r^{(m)}
\end{equation}
where $I^{n+1}$ is the exact solution\footnote{For clarity, in this chapter the exact solution is the
    exact solution to the transport problem defined by Eq.~\eqref{eq:ho_base}, not to the
continuous equations that are trying to be solved.}  to the problem defined by Eq.~\eqref{eq:ho_base} and
${\epsilon}^{(m)}$ is the true error in the approximate solution $\tilde{I}^{n+1,(m)}$. 
The $\B L$ operator in the above equation is inverted with the MC method, which
statistically estimates an LDFE projection of the error in $\tilde{I}^{n+1,(m)}$, i.e., 
\begin{equation}\label{eq:mc_err}
\tilde{\epsilon}^{(m)} = \B L^{-1} r^{(m)}
\end{equation}
where $\B L^{-1}$ is the Monte Carlo inversion of the streaming and removal operator.  
This inversion is strictly a standard Monte Carlo simulation; particle histories are
tracked and the mean behavior estimated as in standard solutions to a Boltzmann transport
equation~\cite{shultic_mc,mcnp}, although the source is
complicated and produces both positive and negative statistical weights; sampling of the
source is detailed in Sec.~\ref{sec:systematic_sampling}.  
It is noted that the exact error in $\tilde{I}^{n+1,(m)}$ (with respect to
Eq.~\eqref{eq:ho_base}) is being estimated with MC;
tallies produce an integral projection of the error onto a LDFE space-angle trial space. 
Volumetric flux tallies over each space-angle element are required to estimate
$\tilde{\epsilon}^{(m)}$, as detailed in Sec.~\ref{sec:tallies}.  
The
space-angle moments of the error computed as $\tilde{\epsilon}^{(m)}$ can be added to the
moments of $\tilde{I}^{n+1}(m)$ to produce a more accurate solution.  

Here, we emphasize the solution $\tilde{I}^{n+1,(m)}$ represents the LDFE projection of the exact Monte Carlo
solution to the transport problem defined by Eq.~\eqref{eq:ho_base}.  The discretization error is in $q$, i.e., the LD spatial
representation of the emission and scattering source and the LDFE space-angle projection $\tilde I^{n}(x,\mu)$.
 The projection of the intensity is in
general far more accurate than a standard finite element solution, e.g., a S$_N$ collocation method in angle.  In typical IMC calculations, the average
energy deposition within a cell is computed using a standard path-length volumetric
flux tally; the zeroth moment of the LDFE projection of ${\epsilon}$ is
computed using an equivalent tally, preserving the zeroth moment of the true error.
To see why the true error is being estimated, it is important to note that 
$\B L$ in Eq.~\eqref{eq:err_eq} is the continuous operator.  The integral equation that
the MC transport method is estimating a solution to in the $\B L^{-1}$ process can be
shown to be the analytic inverse of the operator $\B L$~\cite{shultis_mc,cj_thesis}.  

The ECMC algorithm is
\begin{enumerate}
    \item Initialize the guess for $\tilde{I}^{n+1,(0)}$ to $\tilde{I}^{n}$ or the
        projection of $\tilde{I}^{n+1}$ from the latest HO solve
\item Compute $r^{(m)}$.
\item Perform a MC simulation to obtain $\tilde{\epsilon}^{(m)} = \B L^{-1} r^{(m)}$
\item Compute a new estimate of the intensity $\tilde I^{n+1,(m+1)} = \tilde I^{n+1,(m)}
+ \tilde\epsilon^{(m)}$
\item Repeat steps 2 -- 4 until desired convergence criteria is achieved. 
\end{enumerate}
The initial guess for the angular intensity $I^{n+1,(0)}$ is computed based on the previous solution
for $\tilde{I}^{n}$. This is a critical step in the algorithm; it significantly reduces the required number of
particles per time step because the intensity does not change drastically between time steps in
optically-thick regions.  It is noted that the ECMC batch (steps 1-4 of the
algorithm) results in essentially the same estimate of the solution as the residual
formulation used in~\cite{rmc}.  The primary difference is that our method uses an LDFE trial
space and iterates on the solution estimate by recomputing the residual.

Exponential convergence is obtained if the error $\epsilon$ is reduced each batch.  With each batch, a
better estimate of the solution is being used to compute the new residual, decreasing
the magnitude of the MC residual source at each iteration $m$, relative to the solution
$I^{n+1}$.  Each MC
estimate of the moments of $\epsilon$ still has a statistical uncertainty that is
governed by the standard $1/\sqrt{N}$ convergence rate~\cite{shultis_mc}, for a
particular source $r^{(m)}$, where $N$ is the number of histories performed.  If the statistical estimate of the projection $\tilde\epsilon$ is not sufficiently
accurate, then the iterations would diverge. It is noted that there is statistical correlation across batches because
$I^{n+1,(m+1)}$ and $\epsilon^{(m)}$ are correlated through $I^{n+1,(m)}$ and the MC source $r^{(m)}$.  
A general proof of exponential
convergence for related adaptive MC transport methods is depicted in~\cite{spanier_mc}.  

The statistical uncertainty in moments of $\epsilon^{(m)}$ can be estimated with the sample variance of
histories.  This provides a statistical estimate of moments of the solution estimated in
that batch, because the moments of the error are the error in whatever estimate
$I^{n+1,(m)}$ is, which obey the central limit theorem~\cite{shultis_mc}.  However, care
must be taken with these statistical estimates, as they do not have the usual MC
interpretation of confidence intervals because of correlations.  Explicitly, if a
particular simulation is repeated with different independent random number seeds, the
sample means will not lie within the range the confidence interval that the sample
variance estimated.  Additionally, the number of histories within each batch are likely
too low for the central limit theorem to truly apply.  

\subsection{Adaptive Mesh Refinement}

Because the exact angular intensity does not in general lie within the LDFE trial space, the
iterative estimate of the error will eventually stagnate once the error cannot be sufficiently
represented by a given FE mesh.  An adaptive $h-$refinement algorithm has been
implemented that can be used to allow the system to continue converging towards the
exact solution~\cite{jake,ans_2014}. For TRT problems where absorption-reemission physics dominate, the diffusive and slowly varying
regions of the problem require a less refined angular mesh to capture the solution than typical neutronics
problems.  However, greater spatial resolution is needed due to steep spatial
gradients.   
Once error stagnation has occurred (and mesh refinement has reached a maximum level),
additional histories can be performed with a
fixed residual source to estimate the remaining error in the current solution.  Although the remaining error will
converge statistically at a standard $1/\sqrt{N}$ convergence rate, the remaining
error will be much smaller than for a standard MC simulation, producing a much more
efficient solution method overall.


\section{Implementation of ECMC Finite-Element Space and Tallies}
\label{sec:tallies}

The ECMC solver uses a finite element representation in space and angle. On the
interior of the cell with the $i$-th spatial index and $j$-th angular index, the linear representation is defined as
\begin{equation*}
    \tilde I(x,\mu) = I_{a,ij} + \frac{2}{h_i}I_{x,ij}\left(x-x_i\right) +
    \frac{2}{h_j}I_{\mu,ij}\left(\mu-\mu_j\right), \quad x_\il <  x < x_\ir,\quad
     \mu_\jl \leq \mu \leq \mu_\jr
\end{equation*}
The spatial cell width is $h_i$, the angular width is
$h_j$, the center of the cell is $(x_i,\mu_j)$, and
\begin{align}\label{app1}
    I_{a,ij} &= \frac{1}{h_i h_j} \iint\limits_{\mathcal{D}} I(x,\mu)\, \dd x \dd \mu \\
    I_{x,ij} &= \frac{6}{h_ih_j}\iint\limits_{\mathcal{D}} \left(\frac{x - x_i}{h_{x}}\right)
    I(x,\mu)\, \dd x \dd \mu \\ \label{app2}
    I_{\mu,ij} &= \frac{6}{h_ih_j}\iint\limits_{\mathcal{D}}
     \left(\frac{\mu - \mu_j}{h_{\mu}}\right)
    I(x,\mu)\, \dd x \dd \mu,
\end{align}
where $\mathcal{D}: x_\il \leq  x \leq  x_\ir \times \mu_\jl \leq \mu \leq \mu_\jr$.
%$I_a$ is the cell average intensity, and $I_\mu$ and $I_x$ define the
%the first moment in $\mu$ and $x$ of the intensity, respectively. 
Standard upwinding in space is used to
define $I(\mu)$ on incoming faces. 

During a MC batch, moments of the error are tallied.  The necessary moments of the error are
defined analogously to Eq.'s~\eqref{app1}--\eqref{app2}.
The tallies are evaluated by weighting the particle density with the appropriate
basis function and integrating along the history path through the cell.  For the cell average, the $n$-th
particle makes the contribution
\begin{equation}
   \epsilon^n_{a,ij} = \frac{1}{h_ih_j} \int\limits_{s^n_o}^{s^n_f}  w^n(x,\mu) \dd s,
\end{equation}
where $s_o^n$ and $s_f^n$ are the beginning and end of the $n$-th particle track in the cell and $w(x,\mu)$ is
the weight of the error particle in the MC simulation.  Weight is attenuated exponentially, i.e., $w(x,\mu)\propto
\exp(-\sigma_t|x/\mu|)$.
Substitution of the exponential attenuation of the weight produces the result
\begin{equation}
    \epsilon^n_{a,ij} = \frac{w(x_0,\mu)}{\sigma_t h_i h_j} \left(1 -
    e^{-\sigma_ts^n}\right).
\end{equation}
Here, $w(x_0,\mu)$ is the particle weight at the start of the path and $s^n$ is the
length of the track. The contribution of a
particle track to $\epsilon_x$ is given by
\begin{equation}
    \epsilon^n_{x,ij} = \frac{w(x_0,\mu)}{h_i^2h_j \sigma_t} \left[x_0 - x_f e^{-\sigma_t s^n}
        + \left(\frac{\mu}{\sigma_t} - x_i \right)\left(1-e^{-\sigma_t s^n}\right),
    \right]
\end{equation}
where $x_0$ and $x_f$ are the beginning and ending $x$ coordinates of the $n$-th
path.  The contribution to the first moment in $\mu$ is 
\begin{equation}
    \epsilon^n_{\mu,ij} = \frac{w(x_0,\mu)}{h_{j}^2h_i\sigma_t}\left(\mu -
    \mu_j\right) \left(1 - e^{-\sigma_ts^n}\right),
\end{equation}
where the particle $x$-direction cosine $\mu$ does not change because it is a pure-absorber simulation.
Finally, the moments of the error are simply the average contribution of all particles.

\section{Adaptive Mesh Refinement}
\label{app:refinement}
This section describes the adaptive refinement strategy for the ECMC algorithm.
Detailed equations for performing projections between meshes and computing the residual source on
the refined meshes can be found in~\cite{jake}.  At the end of the ECMC batch,
refinement is performed in space-angle cells based on a jump indicator.  The jump
indicator is the magnitude of the different between $I(x,\mu)$ in adjacent cells,
averaged over each edge.  The value of the largest jump, out of the four edges within a
cell, is used as the
indicator for that cell.  Based on this indicator, the 20\% of cells with the largest jump are
refined.  Future work will explore simply using $\epsilon$ to indicate refinement,
rather than the jump error.  The refinement of a cell is chosen to be symmetric, with each space-angle cell divided into four
equal-sized cells.  The solution for $\tilde{I}^{n+1}(x,\mu)$ of the batch is projected onto
the finer mesh for the next batch. Because the dimensionality of the sample space has
increased, we increase the number of histories per batch s.t. the ratio of the number
of histories to total cells is approximately constant for all meshes.  At the end of the last HO solve in a time step,
$\tilde{I}^{n+1}$ is projected back onto the original, coarsest mesh and stored as
$\tilde{I}^{n}$ for the next time step.


\section{MC solution with LDD trial space}
\label{sec:ldd_mc}

The inclusion of the outflow discontinuity has a minimal effect on the treatment of the
residual source. The residual source and process of estimating moments of
the error on the interior of a space-angle cell is unchanged.  The process of estimating
the solution on the outgoing face requires tallying the solution when particles leave a
cell. The tallying process is discussed later in Section~\ref{sec:face_tallies}.  

Applying $L$ to the LDD trial space, as shown in Fig.~\ref{fig:ldd}, results in two $\delta$ functions at each interior face.
For positive flow, at a face $x_{i+1/2}$, the face portion of the residual is defined as
\begin{align}
    \label{eq:res_face}
    \rface(x_{i+1/2}) &= -\mu \pderiv{\tilde I^{(m)}}{x}\big|_{x_{i+1/2}}\\
    &= \rface(x_{i+1/2}^-)\delta^-(x - x_{i+1/2}) + \rface(x_{i+1/2}^+)\delta^+(x - x_{i+1/2}) 
\end{align}
where
\begin{align}
    \rface(x_{i+1/2}^-) &= -\mu\left( \tilde I^{(m)}(x_{i+1/2},\mu) - \tilde I^{(m)}(x_{i+1/2}^-,\mu)
           \right)\\
    \rface(x_{i+1/2}^+) &= -\mu\left( \tilde I^{(m)}(x_{i+1/2}^+,\mu) -
           \tilde I^{(m)}(x_{i+1/2},\mu)
           \right).
\end{align}
Here, $I^{(m)}(x_{i+1/2}^+)$ and $I^{(m)}(x_{i+1/2}^-)$ are the LD solution extrapolated to $x_{i+1/2}$ from the
$x$ cell $i$ and cell $i+1$, respectively.
Particles sampled from the two $\delta$-functions have the same starting location.  The
only difference is, for positive $\mu$,  particles sampled from $\rface(x^-_{i+1/2})$ will
contribute to the face tally at $x_{i+1/2}$; the opposite is true for negative $\mu$.

To reduce variance, we do not sample the two $\delta$ functions independently.
%or score contributions to the outflow face from the interior face source. 
Instead, we combine the
two $\delta$-functions into a single face source,
do not score particles at the face from which they are sampled.  To account for the
untallied error, we add the analytic
contribution to the error from the face source to the corresponding face at the end of a batch.
It is noted the combination of the two $\delta$-functions produces the same residual source as the
original LD residual.

Define the additional error contribution 
from the face sources at $x_{i+1/2}$ as $\dep$.  This additional error is tallied
everywhere by MC, except for at $x_{i+1/2}$.  The transport equation satisfied by $\dep$, for positive
$\mu$, with effective total cross 
section $\hat \sigma_t$, is
\begin{equation}
    \label{eq:ho_face}
    \mu \pderiv{\dep}{x} + \hat\sigma_t \dep = \rface(x_{i+1/2}^-)\delta^-(x - x_{i+1/2}) + \rface(x_{i+1/2}^+)\delta^+(x - x_{i+1/2}) 
\end{equation}
This equation is integrated from $x_{i+1/2}-\alpha$ to $x_{i+1/2}$ to produce
\begin{multline}
    \mu\dep(x_{i+1/2},\mu) - \mu\dep(x_{i+1/2}-\alpha,\mu)  + \int\limits_{x_{i+1/2}-\alpha}^0 
    \hat \sigma_t \dep \dd x  \\ =  \rface(x_{i+1/2}^-) +
        \int\limits_{x_{i+1/2}-\alpha}^0\rface(x_{i+1/2}^+)\delta^+(x - x_{i+1/2}) \dd x.
\end{multline}
The integral on the right side of the equation is zero because $\delta^+(x-x_{i+1/2})$ is
zero for $(-\infty,x_{i+1/2}]$.  The limit of the above equation is taken as $\alpha\to0$, i.e.,
\begin{multline}
    \lim_{\alpha\to0}\left( \mu\dep(x_{i+1/2},\mu) - \mu\dep(x_{i+1/2}-\alpha,\mu)  + \int\limits_{x_{i+1/2}-\alpha}^0 
    \hat \sigma_t \dep \dd x \right)  = \lim_{\alpha\to0} \rface(x_{i+1/2}^-) 
\end{multline}
The integral goes to zero because $\dep$ is smooth on the interior of the cell, and
$\mu\dep(x_{i+1/2}-\alpha,\mu)$ goes to zero because there is no source upstream of
$x_{i+1/2}^-$. Thus, the final solution is
\begin{equation}
    \dep(x_{i+1/2},\mu) = \frac{\rface(x_{i+1/2}^-)}{\mu} = 
     \tilde I^{(m)}(x_{i+1/2}^-,\mu) - \tilde I^{(m)}(x_{i+1/2},\mu)
.
\end{equation}
The update for $I(x_{i+1/2},\mu)$ is 
\begin{align}
   \tilde I^{(m+1)}(x_{i+1/2},\mu) &= \tilde I^{(m)}(x_{i+1/2},\mu) + \epsilon^{(m)}(x_{i+1/2},\mu) +
    \dep(x_{i+1/2},\mu) \\ 
        &= \tilde I^{(m)}(x_{i+1/2}^-,\mu) + \epsilon^{(m)}(x_{i+1/2},\mu).
\end{align}
This result has the peculiar effect that the estimation of the solution on a face depends only on
the interior solution $\tilde I^{(m)}(x_{i+1/2}^-,\mu)$ and not the previous face value 
$\tilde I^{(m)}(x_{i+1/2},\mu)$. This could be used to only estimate
face values in particular cells, at any chosen batch.






\section{Continuous Weight Deposition Tallies}


As in~\cite{park}, because we are solving a pure absorber problem with Monte Carlo, we will allow
particles to stream without absorption to reduce statistical 
variance in the tallies.  The weight of particles is reduced deterministically along
the path as they stream, with no need to sample a path length.  Because particles are exponentially attenuated, the normalized weight is
adjusted as $w(x,\mu) = w(x_0,\mu)\exp(-\sigma_t|(x-x_0)/\mu|)$, where $x_0$ is the starting location of the path.  The tallies account
for the continuously changing weight, as given in Appendix~\ref{sec:tallies}. Histories are allowed to stream in this manner for 6 mean free paths (mfp))
before switching to analog path length sampling; this limits the tracking of very small weight histories. The choice of 6 mfp allows particles to 
continuously deposit weight until they reach 0.25\% of their original weight.  Path lengths are tracked in terms of mfp, so there is no need to resample at material
interfaces.

\section{Systematic Sampling Algorithm for Residual Source}
\label{sec:systematic_sampling}

This representation can directly be plugged into
Eq.~\eqref{eq:resid} and evaluated to produce the residual source in the ECMC HO transport
problem.  The MC source $r^{(m)}(x,\mu)$ in Eq.~\eqref{eq:mc_err}
consists of both face and volumetric sources and can produce positive and
negative weight particles.  The distribution for sampling particle coordinates, in space and angle, is based on the $L_1$
norm over space and angle of the residual~\cite{jake}.  A particular cell volume or face 
is sampled, and then rejection sampling~\cite{shultis_mc} is used to sample from
the appropriate distribution on the face or interior of the space-angle cell.  If the
residual is negative at the sampled coordinates, the weight of the particle history is negative.

As another way to improve efficiency, a modified systematic sampling
method~\cite{shultis_mc} was used for determining source particle locations.  The goal is
to effectively distribute particle histories to regions of importance, but to sample a
sufficient number of histories in less probable regions to prevent large statistical
noise.  However, there is no need to sample histories in regions in thermal equilibrium.
The residual gives a good indication of where histories are most likely to contribute to
the error, particularly in optically thick cells where particles do not transport long
distances.   In
the sampling algorithm the number of particle histories sampled in each space-angle cell
is predetermined and proportional to the magnitude of the residual, including face and
volumetric sources, within that cell.  Then, for the predetermined number of histories
within a cell, the source location is randomly sampled according to the residual source
distribution of that cell.  In cells where the relative magnitude of the residual is on the order of roundoff no particle histories are sampled. In these 
regions the problem is remaining in equilibrium and the solution is known exactly.  For
cells that are significant, but have a predetermined number of histories below some preset
minimum $N_{min}$, the number of histories sampled in that cell is set to $N_{min}$. This
is to limit bad statistics in low probability cells (this would be important for
adaptively refined meshes).  In the simulations performed for this work $N_{min}=1$.  This
choice was made to keep the total number of histories per time step constant throughout
the simulation for comparison to IMC. 

%The unmodified probability of a particle being born in cell $j$ is 
%\begin{equation}
%p_j = \frac{||r^{(m)}_j||}{||r^{(m)}||}
%\end{equation}
%Thus, the number of
%particles in cell $j$ is 
%\begin{equation}
%N_j = 
%\left\{\begin{matrix}
% \lfloor(Np_j)\rfloor, & Np_j > N_{\min}
%\\ 0, & \frac{p_j}{1/N_c} < p_{cut}
%\\ N_{min}, & \text{else}
%\end{matrix}\right.
%\end{equation}
%where $N_{\min}$ is the minimum number of histories in significant cells, $N_c$  is the number of cells, and $p_{cut}$ is the chosen relative probability cutoff.
 %This is done by first filling the cells with $N_{min}$ histories and distributing the remaining number of histories proportional to $p_j$.

